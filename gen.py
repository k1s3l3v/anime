# -*- coding: utf-8 -*-
"""gen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ePXCa5jlQWme0Oz5hb8nUp-2O9ned5q0
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

char_to_idx={'\t': 95,
 '\n': 11,
 ' ': 0,
 '!': 28,
 '"': 52,
 '#': 79,
 '$': 94,
 '%': 83,
 '&': 87,
 "'": 22,
 '(': 71,
 ')': 70,
 '*': 81,
 '+': 93,
 ',': 24,
 '-': 37,
 '.': 14,
 '/': 78,
 '0': 59,
 '1': 60,
 '2': 63,
 '3': 65,
 '4': 68,
 '5': 66,
 '6': 73,
 '7': 69,
 '8': 72,
 '9': 74,
 ':': 62,
 ';': 84,
 '<': 90,
 '=': 85,
 '>': 10,
 '?': 30,
 '@': 92,
 'A': 35,
 'B': 41,
 'C': 43,
 'D': 40,
 'E': 46,
 'F': 54,
 'G': 50,
 'H': 36,
 'I': 27,
 'J': 55,
 'K': 49,
 'L': 45,
 'M': 38,
 'N': 39,
 'O': 42,
 'P': 48,
 'Q': 64,
 'R': 47,
 'S': 33,
 'T': 31,
 'U': 56,
 'V': 61,
 'W': 32,
 'X': 76,
 'Y': 34,
 'Z': 67,
 '[': 82,
 '\\': 58,
 ']': 77,
 '^': 96,
 '_': 86,
 'a': 4,
 'b': 26,
 'c': 19,
 'd': 15,
 'e': 1,
 'f': 21,
 'g': 18,
 'h': 8,
 'i': 5,
 'j': 44,
 'k': 25,
 'l': 12,
 'm': 17,
 'n': 6,
 'o': 3,
 'p': 23,
 'q': 57,
 'r': 9,
 's': 7,
 't': 2,
 'u': 13,
 'v': 29,
 'w': 20,
 'x': 51,
 'y': 16,
 'z': 53,
 '{': 89,
 '|': 91,
 '}': 88,
 '~': 80,
 '�': 75}
idx_to_char={0: ' ',
 1: 'e',
 2: 't',
 3: 'o',
 4: 'a',
 5: 'i',
 6: 'n',
 7: 's',
 8: 'h',
 9: 'r',
 10: '>',
 11: '\n',
 12: 'l',
 13: 'u',
 14: '.',
 15: 'd',
 16: 'y',
 17: 'm',
 18: 'g',
 19: 'c',
 20: 'w',
 21: 'f',
 22: "'",
 23: 'p',
 24: ',',
 25: 'k',
 26: 'b',
 27: 'I',
 28: '!',
 29: 'v',
 30: '?',
 31: 'T',
 32: 'W',
 33: 'S',
 34: 'Y',
 35: 'A',
 36: 'H',
 37: '-',
 38: 'M',
 39: 'N',
 40: 'D',
 41: 'B',
 42: 'O',
 43: 'C',
 44: 'j',
 45: 'L',
 46: 'E',
 47: 'R',
 48: 'P',
 49: 'K',
 50: 'G',
 51: 'x',
 52: '"',
 53: 'z',
 54: 'F',
 55: 'J',
 56: 'U',
 57: 'q',
 58: '\\',
 59: '0',
 60: '1',
 61: 'V',
 62: ':',
 63: '2',
 64: 'Q',
 65: '3',
 66: '5',
 67: 'Z',
 68: '4',
 69: '7',
 70: ')',
 71: '(',
 72: '8',
 73: '6',
 74: '9',
 75: '�',
 76: 'X',
 77: ']',
 78: '/',
 79: '#',
 80: '~',
 81: '*',
 82: '[',
 83: '%',
 84: ';',
 85: '=',
 86: '_',
 87: '&',
 88: '}',
 89: '{',
 90: '<',
 91: '|',
 92: '@',
 93: '+',
 94: '$',
 95: '\t',
 96: '^'}

class TextRNN(nn.Module):
    
    def __init__(self, input_size, hidden_size, embedding_size, n_layers=1):
        super(TextRNN, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.embedding_size = embedding_size
        self.n_layers = n_layers

        self.encoder = nn.Embedding(self.input_size, self.embedding_size)
        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.n_layers)
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(self.hidden_size, self.input_size)
        
    def forward(self, x, hidden):
        x = self.encoder(x).squeeze(2)
        out, (ht1, ct1) = self.lstm(x, hidden)
        out = self.dropout(out)
        x = self.fc(out)
        return x, (ht1, ct1)
    
    def init_hidden(self, batch_size=1):
        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device),
               torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device))

def evaluate(model, char_to_idx, idx_to_char, start_text=' ', prediction_len=200, temp=0.3):
    hidden = model.init_hidden()
    idx_input = [char_to_idx[char] for char in start_text]
    train = torch.LongTensor(idx_input).view(-1, 1, 1).to(device)
    predicted_text = start_text
    
    _, hidden = model(train, hidden)
        
    inp = train[-1].view(-1, 1, 1)
    
    for i in range(prediction_len):
        output, hidden = model(inp.to(device), hidden)
        output_logits = output.cpu().data.view(-1)
        p_next = F.softmax(output_logits / temp, dim=-1).detach().cpu().data.numpy()        
        top_index = np.random.choice(len(char_to_idx), p=p_next)
        inp = torch.LongTensor([top_index]).view(-1, 1, 1).to(device)
        predicted_char = idx_to_char[top_index]
        predicted_text += predicted_char
    
    return predicted_text

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = TextRNN(input_size=len(idx_to_char), hidden_size=128, embedding_size=128, n_layers=2)
model.to(device)

model.load_state_dict(torch.load('/text_gen'))
model.eval()

start_text = '. '

text = (evaluate(
    model, 
    char_to_idx, 
    idx_to_char, 
    temp=0.3, 
    prediction_len=150, 
    start_text=start_text
    )
)

while text == start_text:
    text = (evaluate(
    model, 
    char_to_idx, 
    idx_to_char, 
    temp=0.3, 
    prediction_len=150, 
    start_text=start_text
    )
)
text

